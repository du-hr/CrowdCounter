{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Crowd_Counter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/du-hr/CrowdCounter/blob/main/Crowd_Counter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwdLP40A4X-A"
      },
      "source": [
        "# ECSE 415 Course Project - Counting People in a Shopping Mall\n",
        "\n",
        "\n",
        "> Final Project of ECSE 415 (Fall 2020) @ McGill University\n",
        "\n",
        "\n",
        "> Authors (G32): Haoran Du (260776911), Daniel Aird (260865951), Carlo D'Angelo (260803454), Jed Lamari-Saysset (260848706)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDjpsH8u6dd1"
      },
      "source": [
        "## **0. Initialization (First run?-> Run, restart runtime, and run again)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ8d3_OC32Hj"
      },
      "source": [
        "#Initialization of Detectron2 + restart runtime\n",
        "!pip install pyyaml==5.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab\n",
        "\n",
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "assert torch.__version__.startswith(\"1.7\")\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n",
        "#exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8onz5lad369I",
        "outputId": "489557b0-b514-42fd-cb02-af8f88e5f2f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhQKetcGX8y",
        "outputId": "fd6e1117-f12f-4f8d-9696-b8c4fc88dd6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import scipy as sci\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "#Import for the Local Binary Pattern\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn import svm\n",
        "from skimage.util.shape import view_as_blocks\n",
        "# ignore the follwoing line if running locally\n",
        "drive.mount('/content/drive')\n",
        "# make path = './' if running locally\n",
        "path = '/content/drive/My Drive/frames/frames/'\n",
        "\n",
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5VYQqenQbc4"
      },
      "source": [
        "## **1. Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcWAWxDGWb5N"
      },
      "source": [
        "### **1.1 Import Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3Db9nba_rY0"
      },
      "source": [
        "Instructions: If this is your first time running the code, uncomment Section 1.1 and Section 1.2 and import the dataset from the images. Then, download the new datasetGray.npz file so you can use that next time. If you already have the datasetGray.npz file saved, just upload it to the session and load it using Section 1.3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3U3KGvMOWxI"
      },
      "source": [
        " images = []\n",
        "\n",
        " # load seq_000001.jpg to seq_000009.jpg\n",
        " for i in range(9):\n",
        "   path_i = path + 'seq_00000' + str(i+1) +'.jpg'\n",
        "   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
        "\n",
        " # load seq_000010.jpg to seq_000099.jpg\n",
        " for i in range(9,99):\n",
        "   path_i = path + 'seq_0000' + str(i+1) +'.jpg'\n",
        "   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
        "\n",
        " # load seq_000100.jpg to seq_000999.jpg\n",
        " for i in range(99,999):\n",
        "   path_i = path + 'seq_000' + str(i+1) +'.jpg'\n",
        "   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
        "\n",
        " # load seq_001000.jpg to seq_002000.jpg\n",
        " for i in range(999,2000):\n",
        "   path_i = path + 'seq_00' + str(i+1) +'.jpg'\n",
        "   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V7Rym-vWRc0"
      },
      "source": [
        "### **1.2 Save Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihpulb94Wg5y"
      },
      "source": [
        "datasetGray = np.asarray(images)\n",
        "#np.savez('datasetGray', datasetGray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWZepYgZ_ftB"
      },
      "source": [
        "### **1.3 Load Saved Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQEN8ds__jwX"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "\n",
        "#Read the datasetGray ndarray instead of importing every image\n",
        "datasetGray = np.load('./datasetGray.npz')['arr_0']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRUoe-A8Wgs2"
      },
      "source": [
        "## **2. Existing Person Detection Models**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqfC4cUhyQ9R"
      },
      "source": [
        "#Configure the detector\n",
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "cfg.MODEL.DEVICE='cpu'\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "#outputs = predictor(im)\n",
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "#print(outputs[\"instances\"].pred_classes)\n",
        "#print(outputs[\"instances\"].pred_boxes)\n",
        "\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "#v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "#out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "#cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64azeamFBFd2",
        "outputId": "a6bc31dc-3e20-4940-87cc-e651e63d73c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "person_images = []\n",
        "not_person_images = []\n",
        "globalMaxWidth = 0\n",
        "globalMaxHeight = 0\n",
        "\n",
        "# 10 SURVEILLANCE IMAGES (UP TO 2000)\n",
        "for p in range(10): \n",
        "  #Get test image from the dataset\n",
        "  im = cv2.cvtColor(datasetGray[p], cv2.COLOR_GRAY2RGB)\n",
        "  outputs = predictor(im)\n",
        "\n",
        "  # box is defined by its top left corner and bottom right corner\n",
        "  boxes_object = outputs[\"instances\"].pred_boxes.clone()\n",
        "  # class 0 is a person\n",
        "  classes_list = outputs[\"instances\"].pred_classes.tolist()\n",
        "\n",
        "  # extract top left corner point and bottom right corner point \n",
        "  # of every box from Boxes object \n",
        "  boxesOfPerson_list = []\n",
        "\n",
        "  for i,j in enumerate(boxes_object.__iter__()):\n",
        "    # only consider boxes with person class\n",
        "    if classes_list[i] == 0:\n",
        "      boxesOfPerson_list.append(j.tolist())\n",
        "  boxesOfPerson = np.array(boxesOfPerson_list)\n",
        "\n",
        "  person_images_list = [] # list of 2D person images that were cropped out\n",
        "\n",
        "  #Find the max and mins of the cropped images to see what our windows should be\n",
        "  box = boxesOfPerson[0]\n",
        "  maxWidth = int(box[2] - box[0]  + 1)\n",
        "  maxHeight = int(box[3] - box[1] + 1)\n",
        "  minWidth = int(box[2] - box[0] + 1)\n",
        "  minHeight = int(box[3] - box[1] + 1)\n",
        "\n",
        "  # crop out the people from image\n",
        "  for k in range(boxesOfPerson.shape[0]):\n",
        "    box = boxesOfPerson[k]\n",
        "    height = int(box[3] - box[1] + 1)\n",
        "    if height > maxHeight: maxHeight = height\n",
        "    if height < minHeight: minHeight = height\n",
        "    width = int(box[2] - box[0] + 1)\n",
        "    if width > maxWidth: maxWidth = width\n",
        "    if width < minWidth: minWidth = width\n",
        "    if maxWidth > globalMaxWidth: globalMaxWidth = maxWidth\n",
        "    if maxHeight > globalMaxHeight: globalMaxHeight = maxHeight\n",
        "    crop_img = im[int(box[1]):int(box[1])+height, int(box[0]):int(box[0])+width]\n",
        "    crop_img = cv2.cvtColor(crop_img, cv2.COLOR_RGB2GRAY)\n",
        "    person_images_list.append(crop_img)\n",
        "    #cv2_imshow(crop_img)\n",
        "  \n",
        "  person_images = person_images + person_images_list\n",
        "\n",
        "  #Now we have the list of people in the image, create random images without people\n",
        "\n",
        "  #Get the random images\n",
        "  notPeople = []\n",
        "  for i in range(100):\n",
        "    #Create random width and height\n",
        "    w = random.randrange(minWidth, maxWidth)\n",
        "    h = random.randrange(minHeight, maxHeight)\n",
        "\n",
        "    #Create random location\n",
        "    x = random.randrange(0, len(im[0])-w)\n",
        "    y = random.randrange(0, len(im)-h)\n",
        "    location = (x, y)\n",
        "\n",
        "    isPerson = False\n",
        "    #Make sure the random location doesnt intersect with the boxes of people\n",
        "    for p in boxesOfPerson:\n",
        "      #If a corner of person is in random window, cancel (4 corners = 4 cases)\n",
        "      if x < p[0] and y < p[1] and x+w > p[0] and y+h > p[1]: isPerson = True\n",
        "      elif x+w > p[2] and y < p[1] and x < p[2] and y+h > p[1]: isPerson = True\n",
        "      elif x < p[0] and y+h > p[3] and x+w > p[0] and y < p[3]: isPerson = True\n",
        "      elif x < p[2] and y < p[3] and x+w < p[2] and y+h > p[3]: isPerson = True\n",
        "      elif x < p[0] and y < p[1] and x+w > p[2] and y+h > p[3]: isPerson = True\n",
        "      #If a corner of random window is in person, cancel (4 corners = 4 cases)\n",
        "      elif p[2] > x+w and p[1] < y and p[0] < x+w and p[3] > y: isPerson = True\n",
        "      elif p[0] < x and p[1] < y and p[2] > x and p[3] > y: isPerson = True\n",
        "      elif p[2] > x and p[1] < y+h and p[0] < x and p[3] > y+h: isPerson = True\n",
        "      elif p[0] < x+w and p[1] < y+h and p[2] > x+w and p[3] > y+h: isPerson = True\n",
        "      #None of the corners interact, but the images still touch (2 cases, if one doesnt work then the other shouldnt either)\n",
        "      elif x < p[0] and y > p[1] and x < p[2] and y < p[3] and x+w > p[0] and y+h > p[1] and x+w > p[2] and y+h < p[3]: isPerson = True\n",
        "      elif p[0] < x and p[1] > y and p[0] < x+w and p[1] < y+h and p[2] > x and p[3] > y and p[2] > x+w and p[3] < y+h: isPerson = True\n",
        "\n",
        "\n",
        "    if not isPerson:\n",
        "      notPerson = Image.fromarray(np.uint8(im)).convert('RGB')\n",
        "      notPerson = notPerson.crop((x, y, x+w, y+h))\n",
        "      #Change to numpy array for consistency\n",
        "      notPerson = np.array(notPerson)\n",
        "      notPerson = cv2.cvtColor(notPerson, cv2.COLOR_RGB2GRAY)\n",
        "      notPeople.append(notPerson)\n",
        "\n",
        "  not_person_images = not_person_images + notPeople\n",
        "\n",
        "  #Lets display the notPeople images\n",
        "  #for notPerson in not_person_images:\n",
        "    #cv2_imshow(notPerson)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-5779a4d1aeee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#Get test image from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetGray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_GRAY2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# box is defined by its top left corner and bottom right corner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"height\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"width\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \"\"\"\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetected_instances\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/modeling/backbone/fpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \"\"\"\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Reverse feature maps into top-down order (from low to high resolution)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mbottom_up_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbottom_up_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/modeling/backbone/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stem\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages_and_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/modeling/backbone/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/layers/wrappers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         x = F.conv2d(\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6odPN_C3sgmr"
      },
      "source": [
        "So at this point we have the list of people cropped out:\n",
        "person_images_list (list of ndarrays)\n",
        "And we have the list of non-people cropped out:\n",
        "notPeople (list of ndarrays)\n",
        "\n",
        "Since i make 100 random locations in the image and then discard the ones that intersect with people, we are left with around 30 images of non-people in the list, this can be increased by just increasing the number of iterations\n",
        "\n",
        "The size of the random images works like this: i looked at the actual people sizes and took the max and min width and height of those windows. I sent made the random windows have width (minWidth, maxWidth) and height (minHeight, maxHeight). This could be a parameter we change to get better results\n",
        "\n",
        "The next thing is: do this with a lot more images than just one, get the features from people and non-people and then i guess send them through SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adMirKpei2OJ"
      },
      "source": [
        "## **3. SVM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIPWM29JBt_W",
        "outputId": "b881c94f-e574-4bd0-e8d9-203fed8ff22c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# image pyramid:  multi-scale representation\n",
        "\n",
        "im_test = cv2.imread(path + 'seq_001073.jpg')\n",
        " \n",
        "print('Original Dimensions : ',im_test.shape)\n",
        "\n",
        "def Pyramid(im_test):\n",
        "  listPyramid = [im_test]\n",
        "  for i in range(3):\n",
        "    scale_percent =  150 # percent of original size\n",
        "    width = int(im_test.shape[1] * scale_percent / 100)\n",
        "    height = int(im_test.shape[0] * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    # resize image\n",
        "    im_test = cv2.resize(im_test, dim, interpolation = cv2.INTER_AREA)\n",
        "    listPyramid.append(im_test)\n",
        "   #plt.imshow(im_test)\n",
        "   #plt.show()\n",
        "  return listPyramid\n",
        "\n",
        "result1 = Pyramid(im_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Dimensions :  (480, 640, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naMFgRj0i_3b"
      },
      "source": [
        "windowSize = 80\r\n",
        "\r\n",
        "# TRAIN IMAGES (from only one surveillance image)\r\n",
        "train_images = person_images + not_person_images\r\n",
        "#print(train_images.shape[0])\r\n",
        "\r\n",
        "# TRAIN LABELS (from only one surveillance image)\r\n",
        "labels_person = np.array([1] * len(person_images)) # person patches have a label of 1\r\n",
        "labels_not_person = np.array([0] * len(not_person_images)) # not person patches have a label of 0\r\n",
        "train_labels = np.concatenate((labels_person, labels_not_person))\r\n",
        "#print(train_labels.shape[0])\r\n",
        "\r\n",
        "\r\n",
        "def pad(image, globalMaxWidth, globalMaxHeight):\r\n",
        "    if globalMaxHeight < globalMaxWidth: globalMaxHeight = globalMaxWidth\r\n",
        "    else: globalMaxWidth = globalMaxHeight\r\n",
        "    result = np.zeros((globalMaxHeight, globalMaxWidth))\r\n",
        "    result = Image.fromarray(np.uint8(result))\r\n",
        "    image = Image.fromarray(np.uint8(image))\r\n",
        "    \r\n",
        "    result.paste(image, (0, 0))\r\n",
        "    result = np.array(result)\r\n",
        "    #cv2_imshow(result)\r\n",
        "    return result\r\n",
        "\r\n",
        "#adding padding to the images to train\r\n",
        "#Based on the max height and width, we need to pad each image with 0's\r\n",
        "def resize(image_list):\r\n",
        "  resizedList = []\r\n",
        "  for i in image_list :\r\n",
        "    shrinkAmount = 100*windowSize/len(i)\r\n",
        "    width = int(i.shape[1] * shrinkAmount / 100)\r\n",
        "    height = int(i.shape[0] * shrinkAmount / 100)\r\n",
        "    dim = (width, height)\r\n",
        "    # resize image\r\n",
        "    resized = cv2.resize(i, dim, interpolation = cv2.INTER_AREA)\r\n",
        "    resized_img = pad(resized, windowSize, windowSize)\r\n",
        "    resizedList.append(resized_img)\r\n",
        "  return resizedList\r\n",
        "\r\n",
        "def HoGFeatures(image_list,img_size, cell_size, block_size, nbins):\r\n",
        "  #Creating the HOGDescriptor\r\n",
        "  hog = cv2.HOGDescriptor(_winSize=(img_size[1] // cell_size[1] * cell_size[1],\r\n",
        "                                  img_size[0] // cell_size[0] * cell_size[0]),\r\n",
        "                        _blockSize=(block_size[1] * cell_size[1],\r\n",
        "                                    block_size[0] * cell_size[0]),\r\n",
        "                        _blockStride=(cell_size[1], cell_size[0]),\r\n",
        "                        _cellSize=(cell_size[1], cell_size[0]),\r\n",
        "                        _nbins=nbins)\r\n",
        "  features = []\r\n",
        "  image_list = np.asarray(image_list)\r\n",
        "  for i in image_list :\r\n",
        "    features.append(hog.compute((i*255).astype(np.uint8)).reshape(1, -1)) #Appending the hogFeatures\r\n",
        "  features = np.vstack(features)\r\n",
        "  return features\r\n",
        "\r\n",
        "\r\n",
        "#Start of SVM Classifier\r\n",
        "resized_train_images = resize(train_images)\r\n",
        "\r\n",
        "print(\"Size of training data images:\" + str(len(resized_train_images[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tl_njucELE4"
      },
      "source": [
        "\n",
        "trainFeatures = HoGFeatures(resized_train_images, (windowSize,windowSize), (8,8), (4,4), 4)\n",
        "\n",
        "#Fitting a non-linear SVM classifier\n",
        "clf = svm.SVC(gamma=0.3, C=700, random_state= None) #Need to change the parameters\n",
        "clf.fit(trainFeatures ,train_labels)\n",
        "\n",
        "#Sliding Window\n",
        "listOfWindows = []\n",
        "locations = []\n",
        "for originalImage in datasetGray[:20:5]:\n",
        "  blocks = view_as_blocks(originalImage, (windowSize,windowSize))\n",
        "  blocks = np.reshape(blocks, (-1, windowSize, windowSize))\n",
        "  blockFeatures = HoGFeatures(blocks, (windowSize,windowSize), (8,8), (4,4), 4)\n",
        "  predictions = clf.predict(blockFeatures)\n",
        "  listOfWindows.append(predictions)\n",
        "\n",
        "\n",
        "\n",
        "#First we need to implement the Local Binary Pattern, for each of the images of people\n",
        "#histogram_data = []\n",
        "#numberOfPoints= 24 #3*8\n",
        "#radius= 3\n",
        "#for image in person_images: #This is only for a list of people\n",
        "#  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "#  LocalBinaryPatternTemp = local_binary_pattern(image, numberOfPoints, radius) #Error because image has 3 dimensions, but should have 2 \n",
        "#  flattenedArray = LocalBinaryPatternTemp.flatten()\n",
        "#  (histograms, bin_edges) = np.histogram(flattenedArray) #There are other parameters, don't understand what they are used for \n",
        "#  histogram_data.append(histograms)\n",
        "\n",
        "#classifier = svm.SVC(gamma=0.3, C=700, random_state = None)\n",
        "#classifier.fit(histogram_data, \"People\" )\n",
        "\n",
        "#Some Tutorials online put the type of the historgram as float and normalize it(To keep in mind)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1htTJ7SJ3I",
        "outputId": "0e65098d-2dbf-45b4-cfa9-425bd3d8f6f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(listOfWindows)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "       1, 1, 1, 1]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       0, 1, 1, 1]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "       0, 1, 1, 0]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "       1, 1, 1, 0])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbZFVFikWsSP"
      },
      "source": [
        "## **3. Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD5KC8MyW1LV"
      },
      "source": [
        "# function that computes the Intersection-over-union (IoU)\r\n",
        "def find_IoU(boxA, boxB):\r\n",
        "  # NUMERATOR\r\n",
        "  # determine the top left corner of the intersection rectangle\r\n",
        "  x_tlc = max(boxA[0], boxB[0])\r\n",
        "  y_tlc = max(boxA[1], boxB[1])\r\n",
        "  # determine the bottom right corner of the intersection rectangle\r\n",
        "  x_brc = min(boxA[2], boxB[2])\r\n",
        "  y_brc = min(boxA[3], boxB[3])\r\n",
        "  # calculate the area of overlap between the predicted bounding box and the ground-truth bounding box\r\n",
        "  overlap_area = max(0, x_brc - x_tlc + 1) * max(0, y_brc - y_tlc + 1)\r\n",
        "  \r\n",
        "  # DENOMINATOR\r\n",
        "  # calculate the area of the predicted bounding box and the area of the ground-truth bounding box separately\r\n",
        "  boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\r\n",
        "  boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\r\n",
        "  # calculate the  area of union\r\n",
        "  # i.e., the area encompassed by both the predicted bounding box and the ground-truth bounding box\r\n",
        "  union_area = boxA_area + boxB_area - overlap_area\r\n",
        "  \r\n",
        "  # IoU\r\n",
        "  # compute the intersection over union\r\n",
        "  iou = overlap_area / float(union_area)\r\n",
        "  return iou"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}