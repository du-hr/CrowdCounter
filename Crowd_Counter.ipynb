{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Crowd_Counter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwdLP40A4X-A"
      },
      "source": [
        "# ECSE 415 Course Project - Counting People in a Shopping Mall\n",
        "\n",
        "\n",
        "> Final Project of ECSE 415 (Fall 2020) @ McGill University\n",
        "\n",
        "\n",
        "> Authors (G32): Haoran Du (260776911), Daniel Aird (260865951), Carlo D'Angelo (260803454), Jed Lamari-Saysset (260848706)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDjpsH8u6dd1"
      },
      "source": [
        "## **0. Initialization (First run?-> Run, restart runtime, and run again)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ8d3_OC32Hj"
      },
      "source": [
        "#Initialization of Detectron2 + restart runtime\n",
        "!pip install pyyaml==5.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab\n",
        "\n",
        "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
        "import torch\n",
        "assert torch.__version__.startswith(\"1.7\")\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n",
        "#exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8onz5lad369I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a97bb80-d342-4fb4-dd49-6967babd63fb"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhQKetcGX8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831f0395-672d-4c1b-d30a-80e6ea40b2d9"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import scipy as sci\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "#Import for the Local Binary Pattern\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn import svm\n",
        "from skimage.util.shape import view_as_blocks\n",
        "# ignore the follwoing line if running locally\n",
        "drive.mount('/content/drive')\n",
        "# make path = './' if running locally\n",
        "path = '/content/drive/My Drive/frames/frames/'\n",
        "\n",
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5VYQqenQbc4"
      },
      "source": [
        "## **1. Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcWAWxDGWb5N"
      },
      "source": [
        "### **1.1 Import Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3Db9nba_rY0"
      },
      "source": [
        "Instructions: If this is your first time running the code, uncomment Section 1.1 and Section 1.2 and import the dataset from the images. Then, download the new datasetGray.npz file so you can use that next time. If you already have the datasetGray.npz file saved, just upload it to the session and load it using Section 1.3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3U3KGvMOWxI"
      },
      "source": [
        "# images = []\n",
        "\n",
        " # load seq_000001.jpg to seq_000009.jpg\n",
        "# for i in range(9):\n",
        "#   path_i = path + 'seq_00000' + str(i+1) +'.jpg'\n",
        "#   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
        "\n",
        "# # load seq_000010.jpg to seq_000099.jpg\n",
        "# for i in range(9,99):\n",
        "#   path_i = path + 'seq_0000' + str(i+1) +'.jpg'\n",
        "#   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
        "\n",
        " # load seq_000100.jpg to seq_000999.jpg\n",
        "# for i in range(99,999):\n",
        "#   path_i = path + 'seq_000' + str(i+1) +'.jpg'\n",
        "#   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
        "\n",
        " # load seq_001000.jpg to seq_002000.jpg\n",
        "# for i in range(999,2000):\n",
        "#   path_i = path + 'seq_00' + str(i+1) +'.jpg'\n",
        "#   images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V7Rym-vWRc0"
      },
      "source": [
        "### **1.2 Save Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihpulb94Wg5y"
      },
      "source": [
        "#datasetGray = np.asarray(images)\n",
        "#np.savez('datasetGray', datasetGray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWZepYgZ_ftB"
      },
      "source": [
        "### **1.3 Load Saved Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQEN8ds__jwX"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "\n",
        "#Read the datasetGray ndarray instead of importing every image\n",
        "datasetGray = np.load('/content/drive/My Drive/datasetGray.npz')['arr_0']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRUoe-A8Wgs2"
      },
      "source": [
        "## **2. Existing Person Detection Models**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqfC4cUhyQ9R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe6443b-1c8f-4ab1-e0b1-a7520fbe48ca"
      },
      "source": [
        "#Configure the detector\n",
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "cfg.MODEL.DEVICE='cpu'\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "#outputs = predictor(im)\n",
        "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
        "#print(outputs[\"instances\"].pred_classes)\n",
        "#print(outputs[\"instances\"].pred_boxes)\n",
        "\n",
        "# We can use `Visualizer` to draw the predictions on the image.\n",
        "#v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "#out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "#cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_final_f10217.pkl: 178MB [00:08, 21.0MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64azeamFBFd2"
      },
      "source": [
        "from skimage.util.shape import view_as_windows\n",
        "\n",
        "person_images = []\n",
        "not_person_images = []\n",
        "globalMaxWidth = 0\n",
        "globalMaxHeight = 0\n",
        "\n",
        "# 10 SURVEILLANCE IMAGES (UP TO 2000)\n",
        "for p in range(0, 2000, 5): \n",
        "  #Get test image from the dataset\n",
        "  im = cv2.cvtColor(datasetGray[p], cv2.COLOR_GRAY2RGB)\n",
        "  outputs = predictor(im)\n",
        "\n",
        "  # box is defined by its top left corner and bottom right corner\n",
        "  boxes_object = outputs[\"instances\"].pred_boxes.clone()\n",
        "  # class 0 is a person\n",
        "  classes_list = outputs[\"instances\"].pred_classes.tolist()\n",
        "\n",
        "  # extract top left corner point and bottom right corner point \n",
        "  # of every box from Boxes object \n",
        "  boxesOfPerson_list = []\n",
        "\n",
        "  for i,j in enumerate(boxes_object.__iter__()):\n",
        "    # only consider boxes with person class\n",
        "    if classes_list[i] == 0:\n",
        "      boxesOfPerson_list.append(j.tolist())\n",
        "  boxesOfPerson = np.array(boxesOfPerson_list)\n",
        "\n",
        "  person_images_list = [] # list of 2D person images that were cropped out\n",
        "\n",
        "  # crop out the people from image\n",
        "  for k in range(boxesOfPerson.shape[0]):\n",
        "    box = boxesOfPerson[k]\n",
        "    #height = int(box[3] - box[1] + 1)\n",
        "    #if height > maxHeight: maxHeight = height\n",
        "    #if height < minHeight: minHeight = height\n",
        "    width = int(box[2] - box[0] + 1)\n",
        "    #if maxHeight > globalMaxHeight: globalMaxHeight = maxHeight\n",
        "    crop_img = im[int(box[1]):int(box[1])+width, int(box[0]):int(box[0])+width]\n",
        "    crop_img = cv2.cvtColor(crop_img, cv2.COLOR_RGB2GRAY)\n",
        "    person_images_list.append(crop_img)\n",
        "    #cv2_imshow(crop_img)\n",
        "  \n",
        "  person_images = person_images + person_images_list\n",
        "\n",
        "  #Now we have the list of people in the image, get images without people\n",
        "\n",
        "\n",
        "\n",
        "  windowSize = 40\n",
        "  step = 2*int(windowSize/3)\n",
        "\n",
        "  #Get every single thing thats not a person\n",
        "  notPeople = []\n",
        "\n",
        "  peopleAndNotPeople = view_as_windows(cv2.cvtColor(im, cv2.COLOR_RGB2GRAY), (windowSize,windowSize), step)\n",
        "  peopleAndNotPeople = np.reshape(peopleAndNotPeople, (-1, windowSize, windowSize))\n",
        "  for i, window in enumerate(peopleAndNotPeople):\n",
        "    isPerson = False\n",
        "    location = ((i%47)*step, (i//47*step))\n",
        "    x = location[0]\n",
        "    y = location[1]\n",
        "    w = windowSize\n",
        "    for p in boxesOfPerson:\n",
        "      #If a corner of person is in random window, cancel (4 corners = 4 cases)\n",
        "      if x < p[0] and y < p[1] and x+w > p[0] and y+w > p[1]: isPerson = True\n",
        "      elif x+w > p[0]+w and y < p[1] and x < p[0]+w and y+w > p[1]: isPerson = True\n",
        "      elif x < p[0] and y+w > p[1]+3 and x+w > p[0] and y < p[1]+w: isPerson = True\n",
        "      elif x < p[0]+w and y < p[1]+w and x+w < p[0]+2 and y+w > p[1]+w: isPerson = True        \n",
        "      elif x < p[0] and y < p[1] and x+w > p[0]+w and y+w > p[1]+w: isPerson = True\n",
        "      #If a corner of random window is in person, cancel (4 corners = 4 cases)\n",
        "      elif p[0]+w > x+w and p[1] < y and p[0] < x+w and p[1]+w > y: isPerson = True\n",
        "      elif p[0] < x and p[1] < y and p[0]+w > x and p[1]+w > y: isPerson = True\n",
        "      elif p[0]+w > x and p[1] < y+w and p[0] < x and p[1]+w > y+w: isPerson = True\n",
        "      elif p[0] < x+w and p[1] < y+w and p[0]+w > x+w and p[1]+w > y+w: isPerson = True\n",
        "      #None of the corners interact, but the images still touch (2 cases, if one doesnt work then the other shouldnt either)\n",
        "      elif x < p[0] and y > p[1] and x < p[0]+w and y < p[1]+w and x+w > p[0] and y+w > p[1] and x+w > p[0]+w and y+w < p[1]+w: isPerson = True\n",
        "      elif p[0] < x and p[1] > y and p[0] < x+w and p[1] < y+w and p[0]+w > x and p[1]+w > y and p[0]+w > x+w and p[1]+w < y+w: isPerson = True\n",
        "\n",
        "    if not isPerson:\n",
        "      notPeople.append(window)\n",
        "\n",
        "  not_person_images = not_person_images + notPeople\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH9DPtCo-EPS"
      },
      "source": [
        "#Lets display the notPeople images\n",
        "#for notPerson in not_person_images:\n",
        "  #cv2_imshow(notPerson)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6odPN_C3sgmr"
      },
      "source": [
        "So at this point we have the list of people cropped out:\n",
        "person_images_list (list of ndarrays)\n",
        "And we have the list of non-people cropped out:\n",
        "notPeople (list of ndarrays)\n",
        "\n",
        "Since i make 100 random locations in the image and then discard the ones that intersect with people, we are left with around 30 images of non-people in the list, this can be increased by just increasing the number of iterations\n",
        "\n",
        "The size of the random images works like this: i looked at the actual people sizes and took the max and min width and height of those windows. I sent made the random windows have width (minWidth, maxWidth) and height (minHeight, maxHeight). This could be a parameter we change to get better results\n",
        "\n",
        "The next thing is: do this with a lot more images than just one, get the features from people and non-people and then i guess send them through SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adMirKpei2OJ"
      },
      "source": [
        "## **3. SVM Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIPWM29JBt_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9520b357-5db9-411a-90a6-11a73557aae2"
      },
      "source": [
        "# image pyramid:  multi-scale representation\n",
        "\n",
        "im_test = cv2.imread(path + 'seq_001073.jpg')\n",
        " \n",
        "print('Original Dimensions : ',im_test.shape)\n",
        "\n",
        "def Pyramid(im_test):\n",
        "  listPyramid = [im_test]\n",
        "  for i in range(3):\n",
        "    scale_percent =  150 # percent of original size\n",
        "    width = int(im_test.shape[1] * scale_percent / 100)\n",
        "    height = int(im_test.shape[0] * scale_percent / 100)\n",
        "    dim = (width, height)\n",
        "    # resize image\n",
        "    im_test = cv2.resize(im_test, dim, interpolation = cv2.INTER_AREA)\n",
        "    listPyramid.append(im_test)\n",
        "   #plt.imshow(im_test)\n",
        "   #plt.show()\n",
        "  return listPyramid\n",
        "\n",
        "result1 = Pyramid(im_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Dimensions :  (480, 640, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxq3D_N5PJQk"
      },
      "source": [
        "#Generating train labels and train images\r\n",
        "# TRAIN IMAGES (from only one surveillance image)\r\n",
        "train_images = person_images + not_person_images\r\n",
        "print(len(train_images))\r\n",
        "\r\n",
        "# TRAIN LABELS (from only one surveillance image)\r\n",
        "labels_person = np.array([1] * len(person_images)) # person patches have a label of 1\r\n",
        "labels_not_person = np.array([0] * len(not_person_images)) # not person patches have a label of 0\r\n",
        "train_labels = np.concatenate((labels_person, labels_not_person))\r\n",
        "#print(train_labels.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwbzPJN-PP-M"
      },
      "source": [
        "#Loading training images \r\n",
        "train_images = np.load('/content/drive/My Drive/train_images.npz')\r\n",
        "#Loading train_labels\r\n",
        "train_labels = np.load('/content/drive/My Drive/train_images.npz')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naMFgRj0i_3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae47875-2d48-49b9-ac7a-03259b2e3526"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def pad(image, globalMaxWidth, globalMaxHeight):\r\n",
        "    if globalMaxWidth < globalMaxWidth: globalMaxWidth = globalMaxWidth\r\n",
        "    else: globalMaxWidth = globalMaxWidth\r\n",
        "    result = np.zeros((globalMaxWidth, globalMaxWidth))\r\n",
        "    result = Image.fromarray(np.uint8(result))\r\n",
        "    image = Image.fromarray(np.uint8(image))\r\n",
        "    \r\n",
        "    result.paste(image, (0, 0))\r\n",
        "    result = np.array(result)\r\n",
        "    #cv2_imshow(result)\r\n",
        "    return result\r\n",
        "\r\n",
        "#adding padding to the images to train\r\n",
        "#Based on the max height and width, we need to pad each image with 0's\r\n",
        "def resize(image_list):\r\n",
        "  resizedList = []\r\n",
        "  for i in image_list :\r\n",
        "    shrinkAmount = 100*windowSize/len(i)\r\n",
        "    width = int(i.shape[1] * shrinkAmount / 100)\r\n",
        "    height = int(i.shape[0] * shrinkAmount / 100)\r\n",
        "    dim = (width, height)\r\n",
        "    # resize image\r\n",
        "    resized = cv2.resize(i, dim, interpolation = cv2.INTER_AREA)\r\n",
        "    resized_img = pad(resized, windowSize, windowSize)\r\n",
        "    resizedList.append(resized_img)\r\n",
        "  return resizedList\r\n",
        "\r\n",
        "def HoGFeatures(image_list,img_size, cell_size, block_size, nbins):\r\n",
        "  #Creating the HOGDescriptor\r\n",
        "  hog = cv2.HOGDescriptor(_winSize=(img_size[1] // cell_size[1] * cell_size[1],\r\n",
        "                                  img_size[0] // cell_size[0] * cell_size[0]),\r\n",
        "                        _blockSize=(block_size[1] * cell_size[1],\r\n",
        "                                    block_size[0] * cell_size[0]),\r\n",
        "                        _blockStride=(cell_size[1], cell_size[0]),\r\n",
        "                        _cellSize=(cell_size[1], cell_size[0]),\r\n",
        "                        _nbins=nbins)\r\n",
        "  features = []\r\n",
        "  image_list = np.asarray(image_list)\r\n",
        "  for i in image_list :\r\n",
        "    features.append(hog.compute((i*255).astype(np.uint8)).reshape(1, -1)) #Appending the hogFeatures\r\n",
        "  features = np.vstack(features)\r\n",
        "  return features\r\n",
        "\r\n",
        "\r\n",
        "#Start of SVM Classifier\r\n",
        "resized_train_images = resize(train_images)\r\n",
        "\r\n",
        "print(\"Size of training data images:\" + str(len(resized_train_images[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127154\n",
            "Size of training data images:40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UKqZMDOfEW9"
      },
      "source": [
        "np.savez('train_images', train_images)\n",
        "np.savez('train_labels', train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tl_njucELE4"
      },
      "source": [
        "trainFeatures = HoGFeatures(resized_train_images, (windowSize,windowSize), (8,8), (4,4), 4)\n",
        "\n",
        "step = int(windowSize/3)\n",
        "\n",
        "sums = []\n",
        "for i in range(60, 2000, 100):\n",
        "  #Fitting a non-linear SVM classifier\n",
        "  clf = svm.SVC(gamma='auto', C=i, random_state=None) #Need to change the parameters\n",
        "  clf.fit(trainFeatures ,train_labels)\n",
        "\n",
        "  #Sliding Window\n",
        "  listOfWindows = []\n",
        "  listOfLocations = []\n",
        "\n",
        "  for originalImage in datasetGray[:10]:\n",
        "    blocks = view_as_windows(originalImage, (windowSize,windowSize), step)\n",
        "    blocks = np.reshape(blocks, (-1, windowSize, windowSize))\n",
        "    blockFeatures = HoGFeatures(blocks, (windowSize,windowSize), (8,8), (4,4), 4)\n",
        "    predictions = clf.predict(blockFeatures)\n",
        "    locations = []\n",
        "    \n",
        "\n",
        "    #Taking note of locations of each person detected\n",
        "    for i in range(len(predictions)):\n",
        "      if predictions[i] == 1:\n",
        "        locations.append(((i%47)*step, (i//47*step)))\n",
        "    listOfLocations.append(locations)\n",
        "    listOfWindows.append(predictions)\n",
        "    sums.append(len(locations))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8PxZEQdRpEr"
      },
      "source": [
        "print(sums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUx9jp1C1zaO"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "\n",
        "#Read the datasetGray ndarray instead of importing every image\n",
        "datasetGray = np.load('/content/drive/My Drive/datasetGray.npz')['arr_0']\n",
        "\n",
        "\n",
        "#Drawing the locations on the image for testing before bounding boxes\n",
        "dotImages = []\n",
        "image = 0\n",
        "for i, image in enumerate(datasetGray[:10]):\n",
        "  for loc in listOfLocations[i]:\n",
        "    loc2 = (loc[0]+windowSize, loc[1]+windowSize)\n",
        "    cv2.rectangle(image,loc, loc2, (0,255,0), 1)\n",
        "  dotImages.append(image) \n",
        "\n",
        "\n",
        "for image in dotImages:\n",
        "  cv2_imshow(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEoU6dcVm6Jx"
      },
      "source": [
        "for i in range(10):\r\n",
        "  print(listOfWindows[i].sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paTonB2ngrmR"
      },
      "source": [
        "import csv\r\n",
        "\r\n",
        "row_list = [['id', 'count']]\r\n",
        "for i, predictions in enumerate(listOfWindows):\r\n",
        "  sum = predictions.sum()\r\n",
        "  row = [i+1, sum]\r\n",
        "  row_list.append(row)\r\n",
        "\r\n",
        "with open('/content/drive/My Drive/submission.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerows(row_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbZFVFikWsSP"
      },
      "source": [
        "## **3. Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD5KC8MyW1LV"
      },
      "source": [
        "# function that computes the Intersection-over-union (IoU)\r\n",
        "def find_IoU(boxA, boxB):\r\n",
        "  # NUMERATOR\r\n",
        "  # determine the top left corner of the intersection rectangle\r\n",
        "  x_tlc = max(boxA[0], boxB[0])\r\n",
        "  y_tlc = max(boxA[1], boxB[1])\r\n",
        "  # determine the bottom right corner of the intersection rectangle\r\n",
        "  x_brc = min(boxA[2], boxB[2])\r\n",
        "  y_brc = min(boxA[3], boxB[3])\r\n",
        "  # calculate the area of overlap between the predicted bounding box and the ground-truth bounding box\r\n",
        "  overlap_area = max(0, x_brc - x_tlc + 1) * max(0, y_brc - y_tlc + 1)\r\n",
        "  \r\n",
        "  # DENOMINATOR\r\n",
        "  # calculate the area of the predicted bounding box and the area of the ground-truth bounding box separately\r\n",
        "  boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\r\n",
        "  boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\r\n",
        "  # calculate the  area of union\r\n",
        "  # i.e., the area encompassed by both the predicted bounding box and the ground-truth bounding box\r\n",
        "  union_area = boxA_area + boxB_area - overlap_area\r\n",
        "  \r\n",
        "  # IoU\r\n",
        "  # compute the intersection over union\r\n",
        "  iou = overlap_area / float(union_area)\r\n",
        "  return iou"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}