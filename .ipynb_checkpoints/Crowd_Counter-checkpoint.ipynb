{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/du-hr/CrowdCounter/blob/danielaird27-patch-1/Crowd_Counter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwdLP40A4X-A"
   },
   "source": [
    "# ECSE 415 Course Project - Counting People in a Shopping Mall\n",
    "\n",
    "\n",
    "> Final Project of ECSE 415 (Fall 2020) @ McGill University\n",
    "\n",
    "\n",
    "> Authors (G32): Haoran Du (260776911), Daniel Aird (260865951), Carlo D'Angelo (260803454), Jed Lamari-Saysset (260848706)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDjpsH8u6dd1"
   },
   "source": [
    "## **0. Initialization (First run?-> Run, restart runtime, and run again)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZ8d3_OC32Hj"
   },
   "outputs": [],
   "source": [
    "#Initialization of Detectron2 + restart runtime\n",
    "!pip install pyyaml==5.1\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version\n",
    "# opencv is pre-installed on colab\n",
    "\n",
    "# install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "import torch\n",
    "assert torch.__version__.startswith(\"1.7\")\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n",
    "#exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8onz5lad369I"
   },
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGhQKetcGX8y",
    "outputId": "b1758019-b344-4776-865e-2278c88acecd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "# ignore the follwoing line if running locally\n",
    "drive.mount('/content/drive')\n",
    "# make path = './' if running locally\n",
    "path = '/content/drive/My Drive/frames/frames/'\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5VYQqenQbc4"
   },
   "source": [
    "## **1. Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWAWxDGWb5N"
   },
   "source": [
    "### **1.1 Import Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3Db9nba_rY0"
   },
   "source": [
    "Instructions: If this is your first time running the code, uncomment Section 1.1 and Section 1.2 and import the dataset from the images. Then, download the new datasetGray.npz file so you can use that next time. If you already have the datasetGray.npz file saved, just upload it to the session and load it using Section 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3U3KGvMOWxI"
   },
   "outputs": [],
   "source": [
    "#images = []\n",
    "\n",
    "# load seq_000001.jpg to seq_000009.jpg\n",
    "#for i in range(9):\n",
    "#  path_i = path + 'seq_00000' + str(i+1) +'.jpg'\n",
    "#  images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "# load seq_000010.jpg to seq_000099.jpg\n",
    "#for i in range(9,99):\n",
    "#  path_i = path + 'seq_0000' + str(i+1) +'.jpg'\n",
    "#  images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "# load seq_000100.jpg to seq_000999.jpg\n",
    "#for i in range(99,999):\n",
    "#  path_i = path + 'seq_000' + str(i+1) +'.jpg'\n",
    "#  images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))\n",
    "\n",
    "# load seq_001000.jpg to seq_002000.jpg\n",
    "#for i in range(999,2000):\n",
    "#  path_i = path + 'seq_00' + str(i+1) +'.jpg'\n",
    "#  images.append(cv2.cvtColor(cv2.imread(path_i), cv2.COLOR_BGR2GRAY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V7Rym-vWRc0"
   },
   "source": [
    "### **1.2 Save Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihpulb94Wg5y"
   },
   "outputs": [],
   "source": [
    "#datasetGray = np.asarray(images)\n",
    "#np.savez('datasetGray', datasetGray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWZepYgZ_ftB"
   },
   "source": [
    "### **1.3 Load Saved Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQEN8ds__jwX"
   },
   "outputs": [],
   "source": [
    "#Read the datasetGray ndarray instead of importing every image\n",
    "datasetGray = np.load('./datasetGray.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRUoe-A8Wgs2"
   },
   "source": [
    "## **2. Existing Person Detection Models**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "bqfC4cUhyQ9R",
    "outputId": "5a676269-dc5c-4750-d672-a408f4cf4741"
   },
   "outputs": [],
   "source": [
    "#First download an image from the COCO dataset\n",
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "im = cv2.imread(\"./input.jpg\")\n",
    "cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt8Eg5L_IbaX"
   },
   "outputs": [],
   "source": [
    "#TEST IMAGE\n",
    "testIm = cv2.imread(path + 'seq_000150.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lpbsq0Pl7DZj"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(testIm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Kx6WNpJ7pqs",
    "outputId": "e9b4a709-1f95-4331-c37b-efc3e0f7b30b"
   },
   "outputs": [],
   "source": [
    "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "print(outputs[\"instances\"].pred_classes)\n",
    "print(outputs[\"instances\"].pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "C_150ZEx7t-y",
    "outputId": "4e80fdc9-f241-40df-9195-bd3f7cac3f05"
   },
   "outputs": [],
   "source": [
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(testIm[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "64azeamFBFd2",
    "outputId": "8566b6f8-329b-4237-f1a5-c9af8c50dffb"
   },
   "outputs": [],
   "source": [
    "# box is defined by its top left corner and bottom right corner\n",
    "boxes_object = outputs[\"instances\"].pred_boxes.clone()\n",
    "# class 0 is a person\n",
    "classes_list = outputs[\"instances\"].pred_classes.tolist()\n",
    "\n",
    "# extract top left corner point and bottom right corner point \n",
    "# of every box from Boxes object \n",
    "boxesOfPerson_list = []\n",
    "for i,j in enumerate(boxes_object.__iter__()):\n",
    "  # only consider boxes with person class\n",
    "  if classes_list[i] == 0:\n",
    "    boxesOfPerson_list.append(j.tolist())\n",
    "boxesOfPerson = np.array(boxesOfPerson_list)\n",
    "\n",
    "\n",
    "person_images_list = [] # list of 2D person images that were cropped out\n",
    "# crop out the people from image\n",
    "for k in range(boxesOfPerson.shape[0]):\n",
    "  box = boxesOfPerson[k]\n",
    "  height = int(box[3] - box[1])\n",
    "  width = int(box[2] - box[0])\n",
    "  crop_img = testIm[int(box[1]):int(box[1])+height, int(box[0]):int(box[0])+width]\n",
    "  person_images_list.append(crop_img)\n",
    "  cv2_imshow(crop_img)\n",
    "person_images = np.array(person_images_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adMirKpei2OJ"
   },
   "source": [
    "## **3. SVM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naMFgRj0i_3b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbZFVFikWsSP"
   },
   "source": [
    "## **3. Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GD5KC8MyW1LV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Crowd_Counter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
